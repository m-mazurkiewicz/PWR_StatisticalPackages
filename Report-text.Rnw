
\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage[top=3cm, bottom=3cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{wrapfig}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<<global_settings, echo=FALSE, warning=FALSE, include = FALSE>>=
library(knitr)
library(car)
library(ggplot2)
library(gridExtra)
library(GGally)
packages<-function(x){
  x<-as.character(match.call()[[2]])
  if (!require(x,character.only=TRUE)){
    install.packages(pkgs=x,repos="http://cran.r-project.org")
    require(x,character.only=TRUE)
  }
}

opts_chunk$set(fig.path='figure/', fig.align='center', fig.pos='H',fig.width=5, fig.height=4)
opts_knit$set(root.dir = getwd())
@
    
  
\SweaveOpts{concordance=TRUE}
  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % strona tytulowa
\title{Statistical packages - report 1}
\author{Urszula Grochocińska, Marcin Mazurkiewicz}
\maketitle
\tableofcontents 

  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\begin{wrapfigure}{l}{0.4\textwidth} 
\vspace{0.2cm}
\begin{center}
\includegraphics[width=0.35\textwidth]{diamondanatomyimproved.jpg}
\caption{Anatomy of diamond. \textit{source: https://www.everything-wedding-rings.com}}
\label{fig:databaseUserTable}
\end{center}
\vspace{0.2pt}
\end{wrapfigure} 

For this report we have decided to analyse "Diamonds", dataset about the features and prices of stones which we have found on Kaggle [https://www.kaggle.com/shivam2503/diamonds]. It contains 54940 records and each one has 11 columns with different variables. First one is irrelevant, because it's just index number. Then we have numerical variables - x, y, z that describe size of diamond in each of three dimensions.There is also weight given in carats. Today, a carat is equal to exactly 0.2 grams). Carat weight is unrelated to the similar sounding karat, which refers to gold's purity. Next we have two variables describing percentage relation between appropriate measures within the diamond.
Depth is the height of a diamond, measured from the culet to the table, divided by its average girdle diameter. Table is the width of the diamond's table expressed as a percentage of its average diameter. The last numerical variable is price. Coming to discrete variables we have cut describing the quality of diamond's cut: Fair, Good, Very Good, Premium, Ideal. The cut describes the symmetry proportioning and polish of the diamond. Then there is color of diamond ranged descending from D to J. The later letter in the alphabet the more yellow. And the last is clarity of particular diamond with possible values: FL,IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3 (ordering from best to worse). It refers to the absence of inclusions and blemishes.
  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Aim of analysis}
  
  Jewelry is a kind of art made of expensive raw material. These art is based on precision and integrity but because of the fact that it is an art sometimes the defects increase the price. These defects make diamond unique and uniqueness is the main part of art. In our analyze we would like to check which features influence the price and at which level. Besides we are interested how important are all features that are influenced by jeweler(shape, cut) and that what are natural (color, clarity).
The figure \ref{dist_price} is a visualisation of distribution of prices. The chart shows that the most common are relativly cheap diamonds. Then if the price increase the diamond becomes rare.

<<reading_data, echo=FALSE>>=
diamonds_data = read.csv("diamonds.csv", header = TRUE)
@

\begin{figure}
\center
<<data_visualisation_price, fig=TRUE, fig.width=11cm , echo=False>>=
ggplot(data=diamonds_data) + geom_histogram(binwidth=200, aes(x=diamonds_data$price))  + xlab("price") + ylab("Frequency") + theme_minimal()

@
\label{dist_price}
\caption{Distributionof values of price in our dataset}
\end{figure}
  
  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data preprocessing}
  This, first step of working with data we started from removing unnecessary variables. We remove the index column, because it is irrelevant information. Then we looked for missing data. Dataset didn't contain any NaNs, but some values were impossible to occur in reality. To handle such situations we changed every zero from x, y or z column to NA, because this will make our upcoming work easier. We also checked if other numerical columns contain zeros, but they didn't. Next,we focused on ensuring that categorical variables don't contain typos. In order to do that we just looked at all values they take, but there wasn't need any cleaning. We didn't deal with outliers, because we will do it within next steps of analysis.


<<data_preprocessing, echo=TRUE, eval=TRUE>>=
diamonds_data <- diamonds_data[2:11] #data cleaning - removing unnecessary variables
#changing 0 to NAs
diamonds_data$z[diamonds_data$z==0] = NA
diamonds_data$x[diamonds_data$x==0] = NA
diamonds_data$y[diamonds_data$y==0] = NA
any(is.na(diamonds_data)) #check if there are any nulls in our dataset
diamonds_data_omited <- na.omit(diamonds_data)
    
#checking for typos
summary(diamonds_data$cut)
summary(diamonds_data$color)
summary(diamonds_data$clarity)
@

Before we started fitting the model whe have made quick look on the data. In figures \ref{dist_continous} and \ref{dist_categorical} we can see the histograms and boxplots. The plot of depth is symetric similar to the bell curve. The next one (x) is much more dispersed. The distributions of y, z and espessialy table shows that this data are have small differences in each group. 


\begin{figure}
\center
<<data_visualisation_continous_variables, fig=TRUE, fig.width=11cm , echo=False>>=
p1 <- ggplot(data=diamonds_data_omited) + geom_histogram(binwidth=0.1, aes(x=diamonds_data_omited$depth))  + xlab("Depth") + ylab("Frequency") + theme_minimal()
p2 <- ggplot(data=diamonds_data_omited) + geom_histogram(binwidth=0.1, aes(x=diamonds_data_omited$x))  + xlab("x") + ylab("") + theme_minimal()
p3 <- ggplot(data=diamonds_data_omited) + geom_histogram(binwidth=0.1, aes(x=diamonds_data_omited$y))  + xlab("y") + ylab("") + theme_minimal()
p4 <- ggplot(data=diamonds_data_omited) + geom_histogram(binwidth=0.1, aes(x=diamonds_data_omited$z))  + xlab("z") + ylab("") + ylab("Frequency") + theme_minimal()
p5 <- ggplot(data=diamonds_data_omited) + geom_histogram(binwidth=0.1, aes(x=diamonds_data_omited$carat))  + xlab("Carat") + ylab("") + theme_minimal()
p6 <- ggplot(data=diamonds_data_omited) + geom_histogram(binwidth=0.1, aes(x=diamonds_data_omited$table))  + xlab("Table") + ylab("") + theme_minimal()
grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2)
@
\label{dist_continous}
\caption{Distributions of each of continous variables}

\end{figure}

\begin{figure}
\begin{center}
<<data_visualisation_categorical_variables, fig.cap = paste('Boxplots of price depending on values of categorical variables'), fig=TRUE , fig.width=11, echo=False>>=
p1 <- qplot(x = cut, y = price, data = diamonds,
      geom = 'boxplot') + coord_cartesian(ylim = c(0, 8000))
p2 <- qplot(x = color, y = price, data = diamonds,
      geom = 'boxplot') + coord_cartesian(ylim = c(0, 8000))
p3 <- qplot(x = clarity, y = price, data = diamonds,
      geom = 'boxplot') + coord_cartesian(ylim = c(0, 8000))
p4 <- ggcorr(diamonds_data_omited)
grid.arrange(p3, p2, p1, p4, nrow = 2)
@
\label{dist_categorical}
\caption{Boxplots of price depending on values of categorical variables and the corelation chart between each two variables}

\end{figure}

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \section{Model fitting}
    \subsection{Continuous variables}
    To fit model to data we use function glmulti from package glmulti. To reduce complexity of model at this step we decided to choose maximum number of used variables to 3. We checked models with different distribution families. These were Gaussian, Poisson and Gamma. It turned out that Gamma model provides the best fit under the BIC criteria. The formula of this model is as follows:
    \begin{equation}
      \text{cost} = -1.0777 \text{carat} + 0.5714 z  + 0.9717 x +  1.0882.
    \end{equation}
    
<<model_simple, fig=TRUE, echo=TRUE, eval=TRUE>>=
model_simple <- glm(formula = price~z+x+carat, data = diamonds_data_omited,
                    family = Gamma(link = 'log'))
plot(model_simple$fitted.values)
points(diamonds_data_omited$price, col = 'red')
@
    
A the plot we can clearly see that there outliers, but we will handle it in next sections.

\subsection{Additional categorical variables}
  In case of additional categorical variables we used very similar approach as in continuous variables. But this time we increased maximum number of variables to 4 and considered Gaussian family. The best model we have found is: 
  \begin{equation}
      \text{cost} = -1016.9 x + 11192.5 \text{carat}  + C_i + D_i - 2826.4,
    \end{equation}
    
    
  \begin{table}[]
  \begin{tabular}{l|llllll}
  color(C_i) & E      & F      & G      & H      & I       & J       \\ \hline
  coefficient  & -214.2 & -282.6 & -486.8 & -993.2 & -1477.5 & -2397.4
  \end{tabular}
  \end{table}
    
    
  \begin{table}[]
  \begin{tabular}{l|lllllll}
  clarity(D_i) & IF     & SI1    & SI2    & VS1    & VS2    & VVS1   & VVS2   \\ \hline
  coefficient  & 5707.1 & 3922.3 & 2952.6 & 4878.2 & 4559.3 & 5339.5 & 5266.3
  \end{tabular}
  \end{table}
  
<<model_middle, fig=TRUE, echo=FALSE, eval=TRUE>>=
model_middle <- glm(formula = price~color+clarity+x+carat, data = diamonds_data_omited,
                    family = gaussian())
plot(model_middle$fitted.values)
points(diamonds_data_omited$price, col = 'red')
@
  
  
\subsection{Model with additional interactions}
  To find model with additional interactions we use very similar approach as in previous cases. Maximum number of variables is 4 and the family is Gaussian. That's how we find the following model:
  
    \begin{equation}
      \text{cost} =  - 6912.8 \text{carat} + 1527.5 \text{carat} \cdot x + C_i + D_i - 783.9,
    \end{equation}
  
  
  \begin{table}[]
  \centering
\begin{tabular}{l|llllll}
carat $\cdot$ color(C_i) & E      & F      & G      & H       & I       & J       \\ \hline
coefficient      & -219.1 & -342.1 & -737.5 & -1373.5 & -1920.8 & -2786.8
\end{tabular}
\end{table}
  
  
  \begin{table}[]
  \centering
\begin{tabular}{l|lllllll}
carat $\cdot$ clarity(D_i) & IF     & SI1    & SI2    & VS1    & VS2    & VVS1   & VVS2   \\ \hline
coefficient        & 6891.3 & 3657.7 & 2664.9 & 4883.4 & 4448.8 & 6154.9 & 5788.6
\end{tabular}
\end{table}

<<model_complex, fig=TRUE, echo=FALSE, eval=TRUE>>=
model_complex <- glm(formula = price~carat + carat:x + carat:color + carat:clarity,
                     data = diamonds_data_omited, family = gaussian())
plot(model_complex$fitted.values)
points(diamonds_data_omited$price, col = 'red')
@
  
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other models}
    To find those three models from previous section we used glmulti function from glmulti package. Thus we take into consideration all models with specific restrictions. We set maximum model size to 3 in case with only continious variables and to 4 in the rest of cases. 
    
    We used glmulti function to find all of considered models. But it turned out, that in second and third case the best model doesn't include z variable. That arouse our curiosity if model excluding this variable is significantly better than with it. Under BIC criteria best model with categorical variables with additional z is only 0.02 \% better than without it. Although it gives better BIC it contains more variables so it may be considered as worse model. 
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Check model assumptions}
    We checked if fitted models fulfill appropriate assumptions. It turns out that none of them do, but we will assume that they do. On the plots below we can see that non of them has normally distributed residuals. Chart 'Residuals vs Fitted' present the distribution of residuals which should be placed around 0. In each of our model the assumption about homogenity is not meet.The normal Q-Q shows if your residuals are normally distributed. Residuals should go around the diagonal line but in ur cases they are not. Cook's distance for residuals helps to detect outliers. They are visible in our cases and we try to handle them in next step of our analysis. Scale location - to describe!!!!!
[Tu bym chciała, żeby pojawiły się tylko wykresy o kótrych wspominamy w tekście]    
<<model_assumptions, fig=TRUE, echo=FALSE, eval=TRUE>>=
layout(matrix(1:4, byrow = T, ncol = 2))
plot(model_simple, which = 1:4)
    
layout(matrix(1:4, byrow = T, ncol = 2))
plot(model_middle, which = 1:4)
    
layout(matrix(1:4, byrow = T, ncol = 2))
plot(model_complex, which = 1:4)
@
    
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outliers handling}
    To find Outliers of our fits we use outlierTest function. It provides us a list of most significant outliers. We decided to delete outliers that are too far from rest of the data. This decision was made because of the fact that this observations are much different then others in only choosen characterisics and it could be the mistake. After that we can try to fit the models again (three lines below but using dataset with deleted outliers - different for each glm or fit by glmulti again, but maybe not necessarily).
<<outliers, echo=FALSE, eval=TRUE>>=
model_simple <- glm(formula = price~z+x+carat, data = diamonds_data_omited, family = Gamma(link = 'log'))
model_middle <- glm(formula = price~color+clarity+x+carat, data = diamonds_data_omited, family = gaussian())
model_complex <- glm(formula = price~carat + carat:x + carat:color + carat:clarity, data = diamonds_data_omited, family = gaussian())
  
outliers_from_simple <- outlierTest(model_simple, n.max = 30) #it computes Bonferroni Outlier Test and prints max values
outliers_indexes_from_simple <- strtoi(labels(outliers_from_simple$p)) #indexes of outliers - we can do sth with them for ex. delete them
  
outliers_from_middle <- outlierTest(model_middle, n.max = 30)
outliers_indexes_from_middle <- strtoi(labels(outliers_from_middle$p))
  
outliers_from_complex <- outlierTest(model_complex, n.max = 30)
outliers_indexes_from_complex <- strtoi(labels(outliers_from_complex$p))

diamonds_data_omited_simplout <- diamonds_data_omited[-c(outliers_indexes_from_simple), ]
model_simple_out <- glm(formula = price~z+x+carat, data = diamonds_data_omited_simplout,
                        family = Gamma(link = 'log'))


@
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Order models}
    It turns out that the best fit we managed to make is with the simplest model with just continuous variables. It becomes even better when we delete outliers. The worst fit of those three models was obtained for model with continuous and categorical variables, but without interactions. The most complex model with additional interactions gave fit somewhere between two previous ones. 

\begin{table}[]
\centering
\begin{tabular}{l|c|c|c}
                                                     & number of variables & AIC    & \begin{tabular}[c]{@{}c@{}}difference between the model \\ and the best model\end{tabular} \\ \hline
Model with only continous variables without outliers & 3                   & 848300 & 0.0\%                                                                                      \\
Model with only continous variables with outliers    & 3                   & 848800 & -0.06\%                                                                                    \\
Model with continous and categorical variables       & 4                   & 912700 & -7.6\%                                                                                     \\
Model with interactions                              & 4                   & 884100 & -4.2\%                                                                                    
\end{tabular}
\end{table}

    
     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Check significance of parameters}
    To check if all parameters in our models are significant we use function summary called at the best  model that we find. Than we look at the significance codes and we can find out what parameters are significant. The all summaries are shown below:
    
<<significance_checking, echo=TRUE, eval=TRUE>>=
summary(model_simple_out)
summary(model_middle)
summary(model_complex)
@
  For two other models we do the same analysis. It turns out that in all models that were fitted by us all parameters are significant. It means that we shouldn't get rid of them.
  
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
    
  
  \end{document}